{
	"metadata": {
		"toc-autonumbering": true,
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# RNDC ETL job",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Install libraries",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import os\nimport sys\n\nos.system(f\"{sys.executable} -m pip install --quiet openpyxl\")\nos.system(f\"{sys.executable} -m pip install --quiet unidecode\")\nos.system(f\"{sys.executable} -m pip install --quiet redshift_connector\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Import libraries",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom awsglue.job import Job\n\nimport pandas as pd\nimport numpy as np\n\nimport redshift_connector\nimport psycopg2\n\nfrom unidecode import unidecode\nimport warnings\nwarnings.filterwarnings('ignore')",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Initial definitions:",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### Variable types:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dict_types = {'MES': int,\n            'COD_CONFIG_VEHICULO': str,\n            'CONFIG_VEHICULO': str,\n            'CODOPERACIONTRANSPORTE': str,\n            'OPERACIONTRANSPORTE':str,\n            'CODTIPOCONTENEDOR': str,\n            'TIPOCONTENEDOR':str,\n            'CODMUNICIPIOORIGEN': int,\n            'MUNICIPIOORIGEN': str,\n            'CODMUNICIPIODESTINO': int,\n            'MUNICIPIODESTINO': str,\n            'CODMERCANCIA': str,\n            'MERCANCIA': str,  \n            'NATURALEZACARGA': str,\n            'VIAJESTOTALES': int,\n            'KILOGRAMOS': float,\n            'GALONES': float,\n            'VIAJESLIQUIDOS': int,\n            'VIAJESVALORCERO': int,\n            'KILOMETROS': float,\n            'VALORESPAGADOS': float,\n            'CODMUNICIPIOINTERMEDIO': int,\n            'MUNICIPIOINTERMEDIO': str,\n            'KILOMETROSREGRESO': float,\n            'KILOGRAMOSREGRESO': float,\n            'GALONESREGRESO': float}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Dimensions and RedShift tables dictionary:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# This dictionary represent all those columns that are necessary to build the dimension tables of the model.\n# The keys represent the code names of the file, the values are a list in which the first element is the description of the code, the second is the dimesion table related to\n# that code and the third one represents the idenfitier column name in the dimension table.\n\ndict_dimensions = {\n                'COD_CONFIG_VEHICULO':['CONFIG_VEHICULO','DIM_CONFIGURACIONES_VEHICULO','ID_CONFIGURACION_VEHICULO'],\n                'CODOPERACIONTRANSPORTE':['OPERACIONTRANSPORTE','DIM_OPERACIONES_TRANSPORTE','ID_OPERACION_TRANSPORTE'],\n                'CODTIPOCONTENEDOR':['TIPOCONTENEDOR','DIM_TIPOS_CONTENEDOR','ID_TIPO_CONTENEDOR'],\n                'CODMUNICIPIOORIGEN':['MUNICIPIOORIGEN','DIM_MUNICIPIOS','ID_MUNICIPIO_ORIGEN'],\n                'CODMUNICIPIODESTINO':['MUNICIPIODESTINO','DIM_MUNICIPIOS','ID_MUNICIPIO_DESTINO'],\n                'CODMERCANCIA':['MERCANCIA','DIM_MERCANCIAS','ID_MERCANCIA'],\n                'CODMUNICIPIOINTERMEDIO':['MUNICIPIOINTERMEDIO','DIM_MUNICIPIOS','ID_MUNICIPIO_INTERMEDIO'],\n                'NATURALEZACARGA':['NATURALEZACARGA','DIM_NATURALEZAS_CARGA','ID_NATURALEZA_CARGA']\n                }",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Fact table columns:",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# This dictionary represent all those columns that are necessary to build the fact table of the model.\n# The keys represent the column names that will be necessary to build the fact table, the values are a list in which the first element is the field order that is\n# going to be used and the second one represents the name of the column in the Redshift table.\n\ndict_fact = {\n            'MES':['MES','ANO_MES'],\n            'COD_CONFIG_VEHICULO':['ID_CONFIGURACION_VEHICULO','ID_CONFIGURACION_VEHICULO'],\n            'CODOPERACIONTRANSPORTE':['ID_OPERACION_TRANSPORTE','ID_OPERACION_TRANSPORTE'],\n            'CODTIPOCONTENEDOR':['ID_TIPO_CONTENEDOR','ID_TIPO_CONTENEDOR'],\n            'CODMUNICIPIOORIGEN':['ID_MUNICIPIO_ORIGEN','ID_MUNICIPIO_ORIGEN'],\n            'CODMUNICIPIODESTINO':['ID_MUNICIPIO_DESTINO','ID_MUNICIPIO_DESTINO'],\n            'NATURALEZACARGA':['ID_NATURALEZA_CARGA','ID_NATURALEZA_CARGA'],\n            'CODMERCANCIA':['ID_MERCANCIA','ID_MERCANCIA'],\n            'VIAJESTOTALES':['VIAJESTOTALES','VIAJES_TOTALES'],\n            'KILOGRAMOS':['KILOGRAMOS','KILOGRAMOS'],\n            'GALONES':['GALONES','GALONES'],\n            'VIAJESLIQUIDOS':['VIAJESLIQUIDOS','VIAJES_LIQUIDOS'],\n            'VIAJESVALORCERO':['VIAJESVALORCERO','VIAJES_VALOR_CERO'],\n            'KILOMETROS':['KILOMETROS','KILOMETROS'],\n            'VALORESPAGADOS':['VALORESPAGADOS','VALORES_PAGADOS'],\n            'CODMUNICIPIOINTERMEDIO':['ID_MUNICIPIO_INTERMEDIO','ID_MUNICIPIO_INTERMEDIO'],\n            'KILOMETROSREGRESO':['KILOMETROSREGRESO','KILOMETROS_REGRESO'],\n            'KILOGRAMOSREGRESO':['KILOGRAMOSREGRESO','KILOGRAMOS_REGRESO'],\n            'GALONESREGRESO':['GALONESREGRESO','GALONES_REGRESO']\n            }\n\nls_fact_keys = list(dict_fact.keys())\nls_fact_values = list(dict_fact.values())\nls_order_values = [x[0] for x in ls_fact_values]\nls_redshift_values = [x[-1] for x in ls_fact_values]\ndict_redshift = {k: v[-1] for k, v in dict_fact.items()}",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Redshift connection:",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "conn = psycopg2.connect(\n            host = 'redshift-cluster-2.cg5i3fotr9gy.sa-east-1.redshift.amazonaws.com', \n            database = 'dev', \n            port = 5439,\n            user = 'admin', \n            password = 'Awscente1803*.*'\n        )\n\ncursor = conn.cursor()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Retrive parameters from Lambda function:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "args = getResolvedOptions(sys.argv, ['bucket','object_key'])\nbucket = args['bucket']\nobject_key = args['object_key']\n\n# bucket = 'rndc-raw'\n# object_key = 'estadisticas/EstadisticasRNDC_202208.xlsx'",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "GlueArgumentError: the following arguments are required: --bucket, --object_key\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Read the dataset",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = pd.read_excel(f\"s3://{bucket}/{object_key}\", dtype=dict_types)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Transformations",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.drop_duplicates(inplace=True)\n\ndf = df.applymap(lambda x: x.upper() if isinstance(x, str) else x)\ndf = df.applymap(lambda x: unidecode(x) if isinstance(x, str) else x)\ndf = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n\ndf['CODMUNICIPIOORIGEN'] = df['CODMUNICIPIOORIGEN'].astype(str)\ndf['CODMUNICIPIODESTINO'] = df['CODMUNICIPIODESTINO'].astype(str)\ndf['CODMUNICIPIOINTERMEDIO'] = df['CODMUNICIPIOINTERMEDIO'].astype(str)\n\nprint(f\"file: {object_key.split('/')[-1]} | status: refined\")",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "file: EstadisticasRNDC_202207.xlsx | status: refined\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Dimesions tables:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "for key in dict_dimensions:\n    \n    # Create dataframe with the coluns required to represent the dimension\n    cod_description = dict_dimensions[key][0]\n    if key != 'NATURALEZACARGA':\n        df_dimension = df[[key,cod_description]]\n    else:\n        df_dimension = df[[key]]\n    df_dimension.drop_duplicates(subset=[key],inplace=True)\n    \n    # Retrieve data from Redshift\n    table_name = dict_dimensions[key][1]\n    query = f'SELECT * FROM {table_name}'\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    column_names = [desc[0] for desc in cursor.description]\n    df_dimension_redshift = pd.DataFrame(rows, columns=column_names)\n    \n    # Identify new values for the dimension\n    if key != 'NATURALEZACARGA':\n        df_merged = pd.merge(df_dimension, df_dimension_redshift, how='left', left_on = key, right_on = 'codigo')\n        df_merged = df_merged[pd.isna(df_merged['codigo'])]\n    else:\n        df_merged = pd.merge(df_dimension, df_dimension_redshift, how='left', left_on = key, right_on = 'naturaleza_carga') # esta dimensión no tiene código\n        df_merged = df_merged[pd.isna(df_merged['naturaleza_carga'])]\n        \n    # Delete right columns\n    left_columns = df_dimension.columns\n    right_columns_to_drop = [col for col in df_merged.columns if col not in left_columns]\n    df_merged = df_merged.drop(columns=right_columns_to_drop)\n    \n    #Insert data if there are new records to be inserted\n    if df_merged.empty != True:\n        data = [tuple(row) for row in df_merged.itertuples(index=False)]\n        ls_col = df_dimension.columns.to_list()\n        ls_col_redshift = df_dimension_redshift.columns.to_list()\n        ls_col_redshift.remove('id')\n        col_text = \",\".join(ls_col_redshift)\n        query = f\"INSERT INTO {table_name} ({col_text}) VALUES ({','.join(['%s']*len(ls_col))})\"\n        cursor.executemany(query, data)\n        conn.commit()\n        print(f'table: {table_name} | inserted new row(s): {len(df_merged)}')\n    else:\n        print(f'table: {table_name} | not new values')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_CONFIGURACIONES_VEHICULO | not new values\n<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_OPERACIONES_TRANSPORTE | not new values\n<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_TIPOS_CONTENEDOR | not new values\n<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_MUNICIPIOS | not new values\n<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_MUNICIPIOS | not new values\n<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_MERCANCIAS | not new values\n<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_MUNICIPIOS | not new values\n<redshift_connector.cursor.Cursor object at 0x7fd184ff9e10>\ntable: DIM_NATURALEZAS_CARGA | not new values\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Fact table:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_fact = df[ls_fact_keys].copy()\n\nfor key in dict_dimensions:\n    #Retrieve data from Redshift\n    table_name = dict_dimensions[key][1]\n    id_name = dict_dimensions[key][-1]\n    \n    if key != 'NATURALEZACARGA': query = f'SELECT id AS {id_name}, codigo FROM {table_name}'\n    else: query = f'SELECT id AS {id_name}, naturaleza_carga AS codigo FROM {table_name}'\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    \n    column_names = [desc[0].upper() for desc in cursor.description]\n    df_dimension_redshift = pd.DataFrame(rows, columns=column_names)\n    \n    #Change cod column to its corresponding id\n    df_fact = pd.merge(df_fact,df_dimension_redshift,left_on = key,right_on = 'CODIGO',how = 'left')\n    df_fact.drop([key,'CODIGO'], axis=1, inplace=True)\n\n#Order de columns to save into Redshift\ndf_fact = df_fact[ls_order_values].copy()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# TEMPORAL\ndf_fact = df_fact.head(50)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "ano_mes = int(df_fact['MES'].unique()[0])\ndataset = list(zip(df_fact['ID_MUNICIPIO_INTERMEDIO'], df_fact['VIAJESVALORCERO'], df_fact['VIAJESLIQUIDOS'], df_fact['VIAJESTOTALES'], df_fact['ID_MERCANCIA'], df_fact['ID_NATURALEZA_CARGA'], df_fact['ID_MUNICIPIO_DESTINO'], df_fact['ID_MUNICIPIO_ORIGEN'], df_fact['ID_TIPO_CONTENEDOR'], df_fact['ID_OPERACION_TRANSPORTE'], df_fact['ID_CONFIGURACION_VEHICULO'], df_fact['MES'], df_fact['GALONESREGRESO'], df_fact['KILOGRAMOSREGRESO'], df_fact['KILOMETROSREGRESO'], df_fact['VALORESPAGADOS'], df_fact['KILOMETROS'], df_fact['GALONES'], df_fact['KILOGRAMOS']))\n\nconsulta = \"DELETE FROM estadisticas WHERE ano_mes = %s;\"\ncursor.execute(consulta, (ano_mes,))\n\nconsulta = \"INSERT INTO estadisticas (id_municipio_intermedio, viajes_valor_cero, viajes_liquidos, viajes_totales, id_mercancia, id_naturaleza_carga, id_municipio_destino, id_municipio_origen, id_tipo_contenedor, id_operacion_transporte, id_configuracion_vehiculo, ano_mes, galones_regreso, kilogramos_regreso, kilometros_regreso, valores_pagados, kilometros, galones, kilogramos) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\ncursor.executemany(consulta, (dataset))\nconn.commit()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "cursor.close()\nconn.close()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}