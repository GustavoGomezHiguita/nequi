{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "## Install libraries",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import os\nimport sys\nos.system(f\"{sys.executable} -m pip install --quiet openpyxl\")\nos.system(f\"{sys.executable} -m pip install --quiet unidecode\")\nos.system(f\"{sys.executable} -m pip install --quiet redshift_connector\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::689317089373:role/AWSGlueServiceRole\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 82072296-7f8f-436b-9d92-096e2bbd00a4\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.3\n--enable-glue-datacatalog true\nWaiting for session 82072296-7f8f-436b-9d92-096e2bbd00a4 to get into ready status...\nSession 82072296-7f8f-436b-9d92-096e2bbd00a4 has been created.\n0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Import libraries",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom awsglue.job import Job\nimport pandas as pd\nimport redshift_connector\nfrom unidecode import unidecode\nimport warnings\nwarnings.filterwarnings('ignore')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Initial definitions:",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### Variable types:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dict_types = {\n            'Aﾃ前MES':int,\n            'NATURALEZA':str,\n            'CODIGO_CARGUE':int,\n            'CODIGO_DESCARGUE': int,\n            'HORAS_VIAJE':float,\n            'HORAS_ESPERA_CARGUE':float,\n            'HORAS_CARGUE':float,\n            'HORAS_ESPERA_DESCARGUE':float,\n            'HORAS_DESCARGUE':float,\n            'CONFIGURACION':str\n            }\nls_order_values = list(dict_types.keys())",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Dimensions and RedShift tables dictionary:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# This dictionary represent all those columns that are necessary to build the dimension tables of the model.\n# The keys represent the code names of the file, the values are a list in which the first element is the dimesion table related to\n# that code and the third one represents the idenfitier column name in the dimension table.\n\ndict_dimensions = {\n                'CONFIGURACION':['DIM_CONFIGURACIONES_VEHICULO','ID_CONFIGURACION_VEHICULO'],\n                'CODIGO_CARGUE':['DIM_MUNICIPIOS','ID_MUNICIPIO_ORIGEN'],\n                'CODIGO_DESCARGUE':['DIM_MUNICIPIOS','ID_MUNICIPIO_DESTINO'],\n                'NATURALEZA':['DIM_NATURALEZAS_CARGA','ID_NATURALEZA_CARGA']\n                }",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Fact table columns:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dict_fact = {\n            'Aﾃ前MES':['Aﾃ前MES','ANO_MES'],\n            'CONFIGURACION':['ID_CONFIGURACION_VEHICULO','ID_CONFIGURACION_VEHICULO'],\n            'NATURALEZA':['ID_NATURALEZA_CARGA','ID_NATURALEZA_CARGA'],\n            'CODIGO_CARGUE':['ID_MUNICIPIO_ORIGEN','ID_MUNICIPIO_ORIGEN'],\n            'CODIGO_DESCARGUE':['ID_MUNICIPIO_DESTINO','ID_MUNICIPIO_DESTINO'],\n            'HORAS_VIAJE':['HORAS_VIAJE','PROMEDIO_HORAS_VIAJE'],\n            'HORAS_ESPERA_CARGUE':['HORAS_ESPERA_CARGUE','PROMEDIO_HORAS_ESPERA_CARGUE'],\n            'HORAS_CARGUE':['HORAS_CARGUE','PROMEDIO_HORAS_CARGUE'],\n            'HORAS_ESPERA_DESCARGUE':['HORAS_ESPERA_DESCARGUE','PROMEDIO_HORAS_ESPERA_DESCARGUE'],\n            'HORAS_DESCARGUE':['HORAS_DESCARGUE','PROMEDIO_HORAS_DESCARGUE']\n            }\nls_fact_keys = list(dict_fact.keys())\nls_fact_values = list(dict_fact.values())\nls_order_values = [x[0] for x in ls_fact_values]\nls_redshift_values = [x[-1] for x in ls_fact_values]\ndict_redshift = {k: v[-1] for k, v in dict_fact.items()}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Redshift connection:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "conn = redshift_connector.connect(\n        host='redshift-cluster-2.cg5i3fotr9gy.sa-east-1.redshift.amazonaws.com',\n        database='dev',\n        port=5439,\n        user='admin',\n        password='Awscente1803*.*',\n        timeout=60\n      )\n\ncursor = conn.cursor()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 87,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Retrive parameters from Lambda function:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# args = getResolvedOptions(sys.argv, ['bucket','object_key'])\n# bucket = args['bucket']\n# object_key = args['object_key']\n\nbucket = 'rndc-raw'\nobject_key = 'tiempos_logisticos/RemesasRNDC_202207.txt'",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Read the dataset",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df = pd.read_csv(f\"s3://{bucket}/{object_key}\",dtype=dict_types, delimiter = '|', usecols=ls_order_values,encoding='latin-1')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Transformations",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.drop_duplicates(inplace=True)\ndf = df.applymap(lambda x: x.upper() if isinstance(x, str) else x)\ndf = df.applymap(lambda x: unidecode(x) if isinstance(x, str) else x)\ndf = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\ndf['CODIGO_CARGUE'] = df['CODIGO_CARGUE'].astype(str)\ndf['CODIGO_DESCARGUE'] = df['CODIGO_DESCARGUE'].astype(str)\ndf_grouped = df.groupby(by=['Aﾃ前MES','NATURALEZA','CODIGO_CARGUE','CODIGO_DESCARGUE','CONFIGURACION'],as_index=False).mean()\nprint(f\"file: {object_key.split('/')[-1]} | status: refined\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "file: RemesasRNDC_202207.txt | status: refined\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Fact table:",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_fact = df_grouped[ls_fact_keys]\n\nfor key in dict_dimensions:\n    # Retrieve data from Redshift\n    table_name = dict_dimensions[key][0]\n    id_name = dict_dimensions[key][-1]\n    if key != 'NATURALEZA':\n        query = f'SELECT id AS {id_name}, codigo FROM {table_name}'\n    else:\n        query = f'SELECT id AS {id_name}, naturaleza_carga AS codigo FROM {table_name}'\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    column_names = [desc[0].upper() for desc in cursor.description]\n    df_dimension_redshift = pd.DataFrame(rows, columns=column_names)\n    \n    # Change cod column to its corresponding id\n    df_fact = pd.merge(df_fact,df_dimension_redshift,left_on = key,right_on = 'CODIGO',how = 'left')\n    df_fact.drop([key,'CODIGO'], axis=1, inplace=True)\n\n# Order de columns to save into Redshift\ndf_fact = df_fact[ls_order_values].copy()\n\n# Insert data\ndata = [tuple(row) for row in df_fact.itertuples(index=False)]\ncol_text = \",\".join(ls_redshift_values)\nquery = f\"INSERT INTO tiempos_logisticos ({col_text}) VALUES ({','.join(['%s']*len(ls_redshift_values))})\"\ncursor.executemany(query, data)\nconn.commit()\nprint(f'table: {table_name} | inserted new row(s): {len(df_fact)}')",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Execution Interrupted. Attempting to cancel the statement (statement_id=87)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "chunk_size = 1000\nnum_chunks = len(df_fact) // chunk_size + 1\n\nfor i in range(num_chunks):\n    start = i * chunk_size\n    end = (i + 1) * chunk_size\n    chunk = df_fact[start:end]\n\n    # Insert data\n    data = [tuple(row) for row in chunk.itertuples(index=False)]\n    col_text = \",\".join(ls_redshift_values)\n    query = f\"INSERT INTO tiempos_logisticos ({col_text}) VALUES ({','.join(['%s']*len(ls_redshift_values))})\"\n    cursor.executemany(query, data)\n    conn.commit()\n    print(f'chunk: {i}/{num_chunks} | table: {table_name} | inserted new row(s): {len(df_fact)}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "cursor.close()\nconn.close()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 86,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}